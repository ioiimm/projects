# work-with-data-in-warehouse
### Проектная работа по интеграции данных нового источника в корпоративное хранилище
- [Ссылка](https://github.com/ioiimm/work-with-data-in-warehouse)
### Описание проекта
- Для привлечения покупателей в маркетплейс добавляется новый источник.
### Результат
- Проект успешно интегрирует новый источник данных к уже имеющимся.
- Реализованы скрипты для переноса данных из источника в хранилище и для инкрементального обновления витрины.
### Структура проекта
- Внутри папки `sripts` расположены DDL- и SQL-запросы.
### Стек технологий
- PostgreSQL

# etl-airflow-project
### Проектная работа по автоматической загрузке данных и обновления витрин
- [Ссылка](https://github.com/ioiimm/etl-airflow-project)
### Описание проекта
- Регулярно формируются готовые показатели для отчётов и дашбордов, чтобы аналитики и менеджеры могли видеть актуальные изменения в трафике и продажах.
### Результат
Сформированы актуальные справочники и реализовано обновление отчётных таблиц с активностью клиентов и ежедневными продажами для анализа и дальнейшего принятия решений.
### Структура проекта
- `dag.py` - DAG с последовательными задачами и расписанием.
### Стек технологий
- Python (psycopg2), Apache Airflow, PostgreSQL

# streaming-service
### Проектная работа по потоковой обработке данных
- [Ссылка](https://github.com/ioiimm/de_projects/tree/main/streaming-service)
### Описание проекта
- Приложение для потоковой обработки данных, которое доставляет пользователям агрегатора для доставки еды уведомления об акциях с ограниченным сроком действия.
- Данные читаются из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени и дополняются данными из PostgreSQL.
### Результат
Проект обеспечивает сбор, обработку и последующую отправку данных в Kafka для push-уведомлений пользователям с подпиской, которая позволяет добавлять рестораны в избранное.
### Структура проекта
- `streaming_service.py` - код Spark Structured Streaming приложения
### Стек технологий
- Apache Spark, Apache Kafka, PostgreSQL, Python (PySpark)

# transaction-metrics
### Проектная работа по аналитическим базам данных в рамках ETL-процесса
- [Ссылка](https://github.com/ioiimm/de_projects/tree/main/transaction-metrics)
### Описание проекта
- Сбор данных по транзакционной активности пользователей из PostgreSQL.
- Обновление таблицы с курсом валют.
- Реализация хранилища в Vertica.
### Результат
Проект успешно решает задачу сбора, обработки и хранения данных из PostgreSQL в Vertica.
### Структура проекта
Внутри `src` расположены папки:
- `/src/dags` - код DAG `1_data_import.py`, который поставляет данные из источника в хранилище. Также код DAG `2_datamart_update.py`, который обновляет витрины данных.
- `/src/sql` - SQL-запрос формирования таблиц в `STAGING`- и `DWH`-слоях, а также скрипт подготовки данных для итоговой витрины.
### Стек технологий
- Apache Airflow, PostgreSQL, Vertica, Python (pandas через PostgresHook, logging, pendulum)
